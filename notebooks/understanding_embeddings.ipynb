{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Embeddings in ChromaDB\n",
    "\n",
    "This notebook explains how embeddings work in our product recommendation system.\n",
    "\n",
    "## What are Embeddings?\n",
    "\n",
    "Embeddings are **numerical vector representations** of text. They convert words, sentences, or documents into arrays of numbers (vectors) that capture semantic meaning.\n",
    "\n",
    "For example:\n",
    "- \"warm winter jacket\" → [0.23, -0.45, 0.67, ..., 0.12] (384 numbers)\n",
    "- \"insulated cold weather coat\" → [0.25, -0.43, 0.65, ..., 0.14] (384 numbers)\n",
    "\n",
    "Similar meanings = Similar vectors (close together in vector space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Embedding Model Used\n",
    "\n",
    "ChromaDB uses **`all-MiniLM-L6-v2`** by default:\n",
    "\n",
    "- **Model**: sentence-transformers/all-MiniLM-L6-v2\n",
    "- **Vector Dimensions**: 384\n",
    "- **Model Size**: ~80MB (downloaded on first use)\n",
    "- **Speed**: Very fast (optimized for CPU)\n",
    "- **Quality**: Good balance of speed and accuracy\n",
    "- **Use Case**: General-purpose semantic similarity\n",
    "\n",
    "This model was automatically downloaded when you first ran `load_products.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection Metadata:\n",
      "Name: outdoor_products\n",
      "Count: 300\n",
      "\n",
      "Metadata: {'description': 'Outdoor apparel and gear products'}\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "# Initialize ChromaDB client\n",
    "client = chromadb.PersistentClient(path=\"../chroma_db\")\n",
    "collection = client.get_collection(name=\"outdoor_products\")\n",
    "\n",
    "# Get the embedding function info\n",
    "print(\"Collection Metadata:\")\n",
    "print(f\"Name: {collection.name}\")\n",
    "print(f\"Count: {collection.count()}\")\n",
    "print(f\"\\nMetadata: {collection.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How the Embedding Process Works\n",
    "\n",
    "### 1. At Index Time (Loading Products)\n",
    "\n",
    "When you ran `load_products.py`:\n",
    "\n",
    "```python\n",
    "collection.add(\n",
    "    documents=[\"NorthPeak Jacket for hiking...\"],  # Text description\n",
    "    metadatas=[{\"brand\": \"NorthPeak\", ...}],\n",
    "    ids=[\"PRD-123\"]\n",
    ")\n",
    "```\n",
    "\n",
    "**What happens:**\n",
    "1. ChromaDB takes the document text\n",
    "2. Passes it through the `all-MiniLM-L6-v2` model\n",
    "3. Gets back a 384-dimensional vector\n",
    "4. Stores: [text, metadata, vector, id]\n",
    "\n",
    "### 2. At Query Time (Searching)\n",
    "\n",
    "When you search:\n",
    "\n",
    "```python\n",
    "collection.query(\n",
    "    query_texts=[\"warm jacket for skiing\"],\n",
    "    n_results=5\n",
    ")\n",
    "```\n",
    "\n",
    "**What happens:**\n",
    "1. ChromaDB takes your query text\n",
    "2. Passes it through the **same** `all-MiniLM-L6-v2` model\n",
    "3. Gets back a 384-dimensional vector\n",
    "4. Compares this vector to all stored product vectors\n",
    "5. Returns the closest matches (by cosine similarity/distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Who Transforms Queries to Embeddings?\n",
    "\n",
    "**ChromaDB does it automatically!**\n",
    "\n",
    "You never manually create embeddings. ChromaDB handles it:\n",
    "\n",
    "- **At index time**: Documents → Embeddings (stored)\n",
    "- **At query time**: Query text → Embeddings (computed on-the-fly)\n",
    "- **Same model used for both** to ensure consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of texts: 3\n",
      "Number of embeddings: 3\n",
      "\n",
      "Embedding vector dimensions: 384\n",
      "\n",
      "First embedding (first 10 values):\n",
      "[-0.09634393  0.11972902  0.00860771  0.09623481  0.06638822  0.04304032\n",
      "  0.09908793 -0.0490774  -0.04074297  0.00528337]\n"
     ]
    }
   ],
   "source": [
    "# Let's manually see how the embedding function works\n",
    "from chromadb.utils.embedding_functions import DefaultEmbeddingFunction\n",
    "\n",
    "# Get the default embedding function (same one ChromaDB uses)\n",
    "embedding_fn = DefaultEmbeddingFunction()\n",
    "\n",
    "# Test text\n",
    "test_texts = [\n",
    "    \"warm winter jacket\",\n",
    "    \"insulated coat for cold weather\",\n",
    "    \"lightweight summer shirt\"\n",
    "]\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = embedding_fn(test_texts)\n",
    "\n",
    "print(f\"Number of texts: {len(test_texts)}\")\n",
    "print(f\"Number of embeddings: {len(embeddings)}\")\n",
    "print(f\"\\nEmbedding vector dimensions: {len(embeddings[0])}\")\n",
    "print(f\"\\nFirst embedding (first 10 values):\")\n",
    "print(embeddings[0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Semantic Similarity\n",
    "\n",
    "Let's see how similar embeddings are for similar text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Calculate cosine similarity between embeddings\n",
    "similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "print(\"Cosine Similarity Matrix:\")\n",
    "print(\"(1.0 = identical, 0.0 = unrelated)\\n\")\n",
    "\n",
    "for i, text1 in enumerate(test_texts):\n",
    "    print(f\"'{text1}':\")\n",
    "    for j, text2 in enumerate(test_texts):\n",
    "        print(f\"  vs '{text2}': {similarity_matrix[i][j]:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Vector Distance\n",
    "\n",
    "ChromaDB returns **distances** (not similarity scores):\n",
    "- Smaller distance = More similar\n",
    "- Distance = 0 = Identical\n",
    "- To convert: `similarity = 1 - distance`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example query showing distances\n",
    "results = collection.query(\n",
    "    query_texts=[\"warm insulated jacket for winter\"],\n",
    "    n_results=5\n",
    ")\n",
    "\n",
    "print(\"Query: 'warm insulated jacket for winter'\\n\")\n",
    "print(\"Results with Distance and Similarity Scores:\\n\")\n",
    "\n",
    "for i, (metadata, distance) in enumerate(zip(results['metadatas'][0], results['distances'][0]), 1):\n",
    "    similarity = 1 - distance\n",
    "    print(f\"{i}. {metadata['product_name']}\")\n",
    "    print(f\"   Distance: {distance:.4f}\")\n",
    "    print(f\"   Similarity: {similarity:.4f}\")\n",
    "    print(f\"   Category: {metadata['subcategory']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Complete Flow\n",
    "\n",
    "### Loading Products (One-time)\n",
    "```\n",
    "Product CSV → Python Dict → Text Description\n",
    "                                    ↓\n",
    "                          all-MiniLM-L6-v2 Model\n",
    "                                    ↓\n",
    "                          384-dim Vector [0.23, -0.45, ...]\n",
    "                                    ↓\n",
    "                          ChromaDB Storage\n",
    "                          [text, metadata, vector, id]\n",
    "```\n",
    "\n",
    "### Searching (Every query)\n",
    "```\n",
    "User Query: \"warm jacket\"\n",
    "         ↓\n",
    "all-MiniLM-L6-v2 Model\n",
    "         ↓\n",
    "Query Vector [0.25, -0.43, ...]\n",
    "         ↓\n",
    "Compare with ALL product vectors\n",
    "(using cosine similarity/distance)\n",
    "         ↓\n",
    "Return top N closest matches\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Different Queries\n",
    "\n",
    "Let's see how the same model handles different types of queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test various query types\n",
    "test_queries = [\n",
    "    \"jacket for cold weather skiing\",\n",
    "    \"lightweight breathable hiking gear\",\n",
    "    \"waterproof rain protection\",\n",
    "    \"casual urban style\",\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Query: '{query}'\")\n",
    "    print('='*60)\n",
    "    \n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=3\n",
    "    )\n",
    "    \n",
    "    for i, (metadata, distance) in enumerate(zip(results['metadatas'][0], results['distances'][0]), 1):\n",
    "        print(f\"\\n{i}. {metadata['product_name']}\")\n",
    "        print(f\"   Similarity: {1-distance:.3f}\")\n",
    "        print(f\"   Purpose: {metadata['primary_purpose']}\")\n",
    "        print(f\"   Features: {metadata['waterproofing']}, {metadata['insulation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Model Comparison\n",
    "\n",
    "ChromaDB's default `all-MiniLM-L6-v2` is great for most use cases, but you can use different models:\n",
    "\n",
    "| Model | Dimensions | Size | Speed | Use Case |\n",
    "|-------|-----------|------|-------|----------|\n",
    "| all-MiniLM-L6-v2 | 384 | 80MB | ⚡⚡⚡ Fast | General purpose (default) |\n",
    "| all-mpnet-base-v2 | 768 | 420MB | ⚡⚡ Medium | Higher quality |\n",
    "| multi-qa-MiniLM-L6-cos-v1 | 384 | 80MB | ⚡⚡⚡ Fast | Q&A/Search optimized |\n",
    "| paraphrase-multilingual | 384 | 470MB | ⚡⚡ Medium | 50+ languages |\n",
    "\n",
    "For your 300 products, the default model is perfect!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Custom Embedding Model (Optional)\n",
    "\n",
    "If you want to use a different model, you can specify it when creating the collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Using a different embedding model (don't run this - just for reference)\n",
    "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
    "\n",
    "# Create custom embedding function\n",
    "custom_ef = SentenceTransformerEmbeddingFunction(\n",
    "    model_name=\"all-mpnet-base-v2\"  # More accurate but slower\n",
    ")\n",
    "\n",
    "# When creating collection, specify the embedding function\n",
    "# collection = client.create_collection(\n",
    "#     name=\"products_with_custom_model\",\n",
    "#     embedding_function=custom_ef\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Embedding Model**: `all-MiniLM-L6-v2` (384 dimensions)\n",
    "2. **Who creates embeddings**: ChromaDB automatically\n",
    "3. **When**: \n",
    "   - At load time: Products → Embeddings (stored)\n",
    "   - At query time: Query → Embeddings (computed)\n",
    "4. **Same model for both**: Ensures consistency\n",
    "5. **You never see the vectors**: ChromaDB handles everything\n",
    "6. **Distance vs Similarity**: Lower distance = Higher similarity\n",
    "\n",
    "## Behind the Scenes\n",
    "\n",
    "```python\n",
    "# What you write:\n",
    "collection.query(query_texts=[\"warm jacket\"], n_results=5)\n",
    "\n",
    "# What ChromaDB does internally:\n",
    "# 1. query_vector = embedding_model(\"warm jacket\")  # [0.25, -0.43, ...]\n",
    "# 2. for each product_vector in database:\n",
    "#       distance = cosine_distance(query_vector, product_vector)\n",
    "# 3. sort by distance (ascending)\n",
    "# 4. return top 5\n",
    "```\n",
    "\n",
    "The magic is: **The model understands that \"warm jacket\" is semantically similar to \"insulated coat\" even though they share no words!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "product-rec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
